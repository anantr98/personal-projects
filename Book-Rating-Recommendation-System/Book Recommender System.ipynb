{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8091325",
   "metadata": {},
   "source": [
    "# Book Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20e371",
   "metadata": {},
   "source": [
    "### Import all necessary libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e650eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916bca5",
   "metadata": {},
   "source": [
    "### Load data and merge. Preprocess the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4396bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/ppt26661059d4g2_gwkrw_8w0000gn/T/ipykernel_42243/3287972686.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books_data = pd.read_csv('archive/Books.csv')\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "books_data = pd.read_csv('archive/Books.csv')\n",
    "ratings_data = pd.read_csv('archive/Ratings.csv')\n",
    "users_data = pd.read_csv('archive/Users.csv')\n",
    "\n",
    "# Ratings data anchors the left side. We join the books after that. Then join the users data.\n",
    "ratings_books = pd.merge(ratings_data, books_data, on='ISBN', how='left')\n",
    "full_data = pd.merge(ratings_books, users_data, on='User-ID', how='left')\n",
    "\n",
    "# Create the mapping of book ID to the book title\n",
    "book_id_to_title = dict(zip(books_data[\"ISBN\"], books_data[\"Book-Title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96df14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upon inspection, there are a lot of null values in the book column which we need to clean out. These represent users that have made a rating. \n",
    "# Drop the values where the book information is null. That is critical information. Age has some null values but we can deal with that later.\n",
    "full_data = full_data.dropna(subset = ['Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0934d18",
   "metadata": {},
   "source": [
    "### Set up the data in surprise format and set up SVD model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e7baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rating scale (0 to 10)\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load data into Surprise format\n",
    "surprise_data = Dataset.load_from_df(full_data[['User-ID', 'ISBN', 'Book-Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71da018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.5068\n",
      "RMSE: 3.5067800052312053\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Split data into training and testing sets\n",
    "trainset, testset = train_test_split(surprise_data, test_size=0.2)\n",
    "\n",
    "# Train the SVD model\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30892f",
   "metadata": {},
   "source": [
    "### Make Personalized Book Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e68195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_n_recommendations(predictions, n=10):\n",
    "    \"\"\"\n",
    "    Get top N recommended books for each user based on predictions.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (list): List of predictions from a Surprise model.\n",
    "        n (int): Number of top recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping user IDs to a list of (book title, predicted rating).\n",
    "    \"\"\"\n",
    "    top_n = defaultdict(list)\n",
    "\n",
    "    # Group predictions by user and store (item_id, estimated rating)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Sort each user's predictions and keep top N\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = sorted(user_ratings, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    # Convert book IDs to book titles\n",
    "    for uid in top_n:\n",
    "        top_n[uid] = [(book_id_to_title.get(iid, \"Unknown Book\"), rating) for iid, rating in top_n[uid]]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "top_recommendations = get_top_n_recommendations(predictions, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eabfb6",
   "metadata": {},
   "source": [
    "### Implement Content-Based Filtering with Nearest Neighbors with book metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "273183ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Neighbor_1  Neighbor_2  Neighbor_3  Neighbor_4  Neighbor_5  Neighbor_6  \\\n",
      "0           0      111977      193923       95231      107299      159200   \n",
      "1           1       69226      130669      187357       48704       73234   \n",
      "2           2       61417      117824       78248      120167      187435   \n",
      "3           3      117371      184279       75775       45833       70324   \n",
      "4           4       54649      149697      214660      266528      214662   \n",
      "\n",
      "   Neighbor_7  Neighbor_8  Neighbor_9  Neighbor_10  ...  Distance_4  \\\n",
      "0      190381       57665      118542       113385  ...    0.347922   \n",
      "1       72119       69955      140651       268831  ...    0.373861   \n",
      "2      236521      192655      115309       143565  ...    0.545645   \n",
      "3      187644      216926      173735        70014  ...    0.393995   \n",
      "4      266527       49069      126385       118655  ...    0.426277   \n",
      "\n",
      "   Distance_5  Distance_6  Distance_7  Distance_8  Distance_9  Distance_10  \\\n",
      "0    0.446818    0.460511    0.460511    0.460511    0.460511     0.460511   \n",
      "1    0.428127    0.432977    0.445305    0.456519    0.456519     0.458068   \n",
      "2    0.601600    0.608582    0.625417    0.632499    0.633400     0.633400   \n",
      "3    0.552098    0.552098    0.564045    0.573874    0.574628     0.574761   \n",
      "4    0.426277    0.426277    0.426277    0.426277    0.426277     0.426277   \n",
      "\n",
      "   Distance_11     Book ID                                         Book Title  \n",
      "0     0.460511  0195153448                                Classical Mythology  \n",
      "1     0.467420  0002005018                                       Clara Callan  \n",
      "2     0.636909  0060973129                               Decision in Normandy  \n",
      "3     0.575723  0374157065  Flu: The Story of the Great Influenza Pandemic...  \n",
      "4     0.428503  0393045218                             The Mummies of Urumchi  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # TfIdf instance\n",
    "from sklearn.metrics.pairwise import cosine_similarity # using content metadata to teach us about ratings\n",
    "\n",
    "# Deal with any null values in the the \"Author\" and \"Publisher\" fields\n",
    "books_data['Book-Author'] = books_data['Book-Author'].fillna('Unknown')\n",
    "books_data['Publisher'] = books_data['Publisher'].fillna('Unknown')\n",
    "\n",
    "# Append book metadata (authors + publishers) into one column\n",
    "books_data['metadata'] = books_data['Book-Author'] + \" \" + books_data['Publisher']\n",
    "\n",
    "# Convert the metadata into the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(books_data[\"metadata\"])\n",
    "\n",
    "# Here I would normally use cosine similarity to calculate tfidf_matrix x tfidf_matrix but the operation is too \n",
    "# expensive. I've chosen to use a K-Nearest-Neighbors route to limit the amount of operations the kernel has to do.\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Use NearestNeighbors to find the most similar books\n",
    "knn = NearestNeighbors(n_neighbors=11, metric='cosine', n_jobs=-1)  # n_jobs=-1 uses all CPUs\n",
    "knn.fit(tfidf_matrix)\n",
    "\n",
    "# Find the nearest neighbors for each book (book_id=0, book_id=1, etc.)\n",
    "distances, indices = knn.kneighbors(tfidf_matrix)\n",
    "\n",
    "# indices now contains the most similar books for each book in the dataset\n",
    "\n",
    "# Create a DataFrame with the indices and distances\n",
    "neighbors_df = pd.DataFrame(indices, columns=[f'Neighbor_{i+1}' for i in range(indices.shape[1])])\n",
    "# Add distances as additional columns in the DataFrame\n",
    "distances_df = pd.DataFrame(distances, columns=[f'Distance_{i+1}' for i in range(distances.shape[1])])\n",
    "\n",
    "# Concatenate both DataFrames\n",
    "knn_df = pd.concat([neighbors_df, distances_df], axis=1)\n",
    "\n",
    "# If you want to add the book titles (assuming you have a 'books_data' DataFrame with book titles)\n",
    "knn_df['Book ID'] = books_data['ISBN'].values  # Assuming you have a 'book_id' column\n",
    "knn_df['Book Title'] = books_data['Book-Title'].values\n",
    "\n",
    "# Show the DataFrame with book IDs, book titles, and their nearest neighbors\n",
    "knn_df.to_csv('knn_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c0a370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendations(user_id, top_n=5):\n",
    "    \"\"\"\n",
    "    Generate hybrid recommendations for a user using SVD and content-based filtering.\n",
    "    \n",
    "    1. Get top-N predictions from SVD.\n",
    "    2. If fewer than N books exist, use content-based filtering to fill the gap.\n",
    "    \"\"\"\n",
    "    svd_recommendations = top_recommendations.get(user_id, []) # Get SVD-based recommendations\n",
    "    \n",
    "    if len(svd_recommendations) >= top_n:\n",
    "        return svd_recommendations[:top_n] # Use SVD only if we have enough data (top_n)\n",
    "    \n",
    "    # Otherwise, use content-based recommendations to fill the gap (SVD + TF-IDF)\n",
    "    recommended_books = set(book_id for book_id, _ in svd_recommendations) # Avoid duplicates\n",
    "    additional_recommendations = []\n",
    "    \n",
    "    for book_title, _ in svd_recommendations:\n",
    "        book_id = books_data[books_data['Book-Title'] == book_title][\"ISBN\"].values[0]\n",
    "        similar_books = knn_df['Book ID'].index.tolist()\n",
    "        \n",
    "        # Add similar books that weren't already recommended\n",
    "        for similar_book in similar_books:\n",
    "            if similar_book not in recommended_books and len(additional_recommendations) < (top_n - len(svd_recommendations)):\n",
    "                #additional_recommendations.append((book_id_to_title[similar_book], 0)) # content-based scores aren't numerical\n",
    "                additional_recommendations.append((book_title,0))\n",
    "        final_recommendations = svd_recommendations + additional_recommendations\n",
    "        \n",
    "        return final_recommendations[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78c031",
   "metadata": {},
   "source": [
    "### Print the recommendations for a specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73f1b471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Love You Forever', 2.5157304450421107),\n",
       " ('Good in Bed', 2.0630823296815324),\n",
       " (\"Chicken Soup for the Woman's Soul (Chicken Soup for the Soul Series (Paper))\",\n",
       "  1.8851332733481787),\n",
       " (\"The Berenstain Bears' New Baby (Pictureback Series)\", 1.66732975848313),\n",
       " ('The Phantom Tollbooth', 1.5545600679413258)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the user ID and how many books you want recommended.\n",
    "hybrid_recommendations(198711, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
